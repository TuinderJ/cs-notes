# What is an algorithm?
- [[#Definition]]
- [[#Analyzing Algorithms]]
	- [[#Constant Time]]
	- [[#Logarithmic Time]]
	- [[#Linear Time]]
	- [[#Log-Linear Time]]
	- [[#Quadratic Time]]
	- [[#Cubic Time]]
	- [[#Exponential Time]]
- [[#Best Case vs. Worst-Case Complexity]]
- [[#Space Complexity]]
- [[#Why is This Important?]]
- [[#Vocabulary]]
- [[#Challenge]]

## Definition
A sequence of steps that solves a problem.

***<u>Requirements</u>***
- Definiteness -> Steps are clear, concise and unambiguous.
- Effectiveness -> You can perform each operation precisely to solve the problem.
- Finiteness -> The algorithm stops after a finite number of steps.
- Correctness -> The same input should produce the same output.

***<u>Exceptions to Requirements</u>***
Sometimes, there are exceptions to these requirements
These exceptions include:
- Expecting a different output when given the same input (RNG)
- Algorithms designed to estimate the output

It is important to be able to determine if one algorithm is better than another. If you can't do that, you'll be much less successful as a programmer.
## Analyzing Algorithms
Algorithms can be judged in multiple ways. Run time is how fast an algorithm works. You might think this is a good way of judging an algorithm but it's not. Run time is heavily dependent on the hardware at the time the algorithm is running.

The better way to judge an algorithm is by its complexity.

Complexity -> The number of steps required to complete the algorithm

The total number of steps is not always feasible. When some steps run under certain conditions, the number of steps changes.

The absolute number of steps is described in a formula `f(n) = n` or `T(n) = n`.
n is used to describe the number of steps the algorithm will take based on a given input.
The total number of steps isn't very relevant when looking at an algorithm's complexity. For example: if you have nested loops, your complexity grows exponentially. This would dwarf any linear steps that your algorithm would take `n^3 > n`.

This is where big O notation come in. Big O notation is used to create an order-of-magnitude function from T(n). Sorted from best (most efficient) to worst (least efficient):
- [[#Constant Time]]
- [[#Logarithmic Time]]
- [[#Linear Time]]
- [[#Log-Linear Time]]
- [[#Quadratic Time]]
- [[#Cubic Time]]
- [[#Exponential Time]]
### Constant Time
`O(1)`
No matter how large your dataset is, the algorithm will take the same number of steps to complete. Giving a free book to your first customer takes the same number of steps for 1 customer and 1 billion customers.
### Logarithmic Time
`O(log n)`
An algorithm that takes logarithmic time gradually increases the time required to complete. A binary search is an example of this. It achieves this by discarding data during its search. it judges data that it doesn't know based on what it does know. That means it doesn't need to look at each piece of data to find what it's looking for.
### Linear Time
`O(n)`
An algorithm that runs in linear time grows at the same rate as the size of the problem. For the book store, if you instead gave your customers a free book if their name started with a `B` and your list of names was not sorted alphabetically, you would need to check each name individually with no exceptions. For 5 customers, you check 5 names. For 400 customers, you check 400 names.
### Log-Linear Time
`O(n log n)`
An algorithm that runs in log-linear time grows in combination (multiplication) of logarithmic and linear time complexities. Imagine evaluating an `O(log n)` operations `n` times. Some sorting algorithms pile data together and go through the data multiple times gaining precision on each pass. This provides multiple `O(log n)` operations (which are more efficient) that are run `n` times.
### Quadratic Time
`O(n**2)`
An algorithm runs in quadratic time when its performance is directly proportional to the problem's size squared.
```
numbers = [1, 2, 3, 4, 5]
for i in numbers:
	for j in numbers:
		x = i * j
		print(x)
```
The easiest example is when you have 2 nested for loops. In this example, numbers are multiplied and printed `n*n` times or `n**2`. The number of steps increases greatly as the size of the data increases.
### Cubic Time
An algorithm runs in cubic time when its performance is directly proportional to the problem's size cubed.
```
numbers = [1, 2, 3, 4, 5]
for i in numbers:
	for j in numbers:
		for h in numbers:
			x = i + j + h
			print(x)
```
The easiest example is when you have 3 nested for loops. Quadratic time and cubic time both run in the *polynomial time scale*. Polynomial time operations use `O(n**a)` to describe their complexity. Polynomial operations should be avoided when it's possible as they can become very slow as the data starts to grow.
### Exponential Time

## Best Case vs. Worst-Case Complexity
## Space Complexity
## Why is This Important?
## Vocabulary
## Challenge